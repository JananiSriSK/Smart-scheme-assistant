{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "9FXFfvrpxbse"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import time, json, os\n",
    "from tqdm import tqdm\n",
    "import google.generativeai as genai\n",
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the key\n",
    "api_key = os.getenv(\"GEMINI_API_KEY\")\n",
    "\n",
    "# Now use it\n",
    "import google.generativeai as genai\n",
    "genai.configure(api_key=api_key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "-ePgMnQbxbhF"
   },
   "outputs": [],
   "source": [
    "# Setup\n",
    "genai.configure(api_key= api_key)\n",
    "model = genai.GenerativeModel('gemini-2.5-pro')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KG0jqnsb86dk"
   },
   "source": [
    "# prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "QKQ4KcRy84mg"
   },
   "outputs": [],
   "source": [
    "template = \"\"\"\n",
    "You are an expert assistant extracting structured eligibility features from government scheme descriptions.\n",
    "\n",
    "Your task is to extract the following features from the given eligibility text and return a valid one-line JSON.\n",
    "\n",
    "Eligibility Text:\n",
    "\"{eligibility}\"\n",
    "\n",
    "Instructions:\n",
    "- For each key below, return:\n",
    "    - `true` if the feature is clearly mentioned.\n",
    "    - `false` if the feature is clearly *not* applicable.\n",
    "    - `null` if the feature is not mentioned at all.\n",
    "- Do not guess or assume. Be precise.\n",
    "- If age or income values are mentioned, extract them as integers.\n",
    "- If not mentioned, return `null`.\n",
    "- Return ONLY a compact, valid one-line JSON. No explanation, no formatting like Markdown or code blocks.\n",
    "\n",
    "Extract the following keys:\n",
    "{{\n",
    "  \"is_student\": bool or null,\n",
    "  \"is_disabled\": bool or null,\n",
    "  \"is_female\": bool or null,\n",
    "  \"is_male\": bool or null,\n",
    "  \"is_transgender\": bool or null,\n",
    "  \"is_girl_child\": bool or null,\n",
    "  \"is_sc_st\": bool or null,\n",
    "  \"is_obc\": bool or null,\n",
    "  \"is_bpl\": bool or null,\n",
    "  \"is_farmer\": bool or null,\n",
    "  \"is_unemployed\": bool or null,\n",
    "  \"is_self_employed\": bool or null,\n",
    "  \"is_salaried_employee\": bool or null,\n",
    "  \"is_daily_wage_worker\": bool or null,\n",
    "  \"is_job_seeker\": bool or null,\n",
    "  \"is_worker\": bool or null,\n",
    "  \"is_labour\": bool or null,\n",
    "  \"is_school_student\": bool or null,\n",
    "  \"is_college_student\": bool or null,\n",
    "  \"has_graduated\": bool or null,\n",
    "  \"is_dropout\": bool or null,\n",
    "  \"is_rural_resident\": bool or null,\n",
    "  \"is_urban_resident\": bool or null,\n",
    "  \"is_migrant_worker\": bool or null,\n",
    "  \"is_single_woman\": bool or null,\n",
    "  \"has_large_family\": bool or null,\n",
    "  \"belongs_to_minority\": bool or null,\n",
    "  \"has_bank_account\": bool or null,\n",
    "  \"no_asset_ownership\": bool or null,\n",
    "  \"receives_pension\": bool or null,\n",
    "  \"has_chronic_illness\": bool or null,\n",
    "  \"is_maternal_beneficiary\": bool or null,\n",
    "  \"is_widow\": bool or null,\n",
    "  \"is_senior_citizen\": bool or null,\n",
    "  \"is_orphan\": bool or null,\n",
    "  \"is_child_of_single_parent\": bool or null,\n",
    "  \"is_woman_headed_household\": bool or null,\n",
    "  \"has_disabled_family_member\": bool or null,\n",
    "  \"lives_in_slum\": bool or null,\n",
    "  \"is_homeless\": bool or null,\n",
    "  \"has_no_house\": bool or null,\n",
    "  \"has_pukka_house\": bool or null,\n",
    "  \"has_kutcha_house\": bool or null,\n",
    "  \"is_tribal\": bool or null,\n",
    "  \"belongs_to_fisherfolk\": bool or null,\n",
    "  \"belongs_to_weaver_community\": bool or null,\n",
    "  \"is_ex_serviceman\": bool or null,\n",
    "  \"is_person_with_hiv\": bool or null,\n",
    "  \"is_victim_of_abuse\": bool or null,\n",
    "  \"is_first_generation_learner\": bool or null,\n",
    "  \"has_family_member_in_govt_service\": bool or null,\n",
    "  \"receives_existing_govt_benefits\": bool or null,\n",
    "  \"annual_income\": integer or null,\n",
    "  \"min_age\": integer or null,\n",
    "  \"max_age\": integer or null,\n",
    "  \"max_income\": integer or null\n",
    "}}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "VJUjBIrUxkYF"
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"Cleaned_schemes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "riSIdLLjztQJ"
   },
   "outputs": [],
   "source": [
    "# Ensure no missing eligibility\n",
    "df = df[df[\"Eligibility\"].notna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "_CMJTCYB0FXf",
    "outputId": "bed02be5-71a5-4dee-96c6-7ff18f6fe466"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing batch 2/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [50:41<00:00,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved extracted_features_batch2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = 1  # batch index\n",
    "chunk = chunks[i]\n",
    "print(f\"\\n Processing batch {i+1}/{len(chunks)}...\")\n",
    "\n",
    "# Check for previous progress\n",
    "output_file = f\"extracted_features_batch{i+1}.csv\"\n",
    "fail_file = f\"failed_prompts_batch{i+1}.csv\"\n",
    "\n",
    "already_done = []\n",
    "if os.path.exists(output_file):\n",
    "    existing = pd.read_csv(output_file)\n",
    "    already_done = existing.index.tolist()\n",
    "    print(f\" Resuming: {len(already_done)} rows already processed.\")\n",
    "\n",
    "results = []\n",
    "fail_log = []\n",
    "\n",
    "for idx, row in tqdm(chunk.iterrows(), total=len(chunk)):\n",
    "    if idx in already_done:\n",
    "        continue  # Skip already processed rows\n",
    "\n",
    "    eligibility = str(row[\"Eligibility\"])\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            prompt = template.format(eligibility=eligibility)\n",
    "            response = model.generate_content(prompt)\n",
    "            raw_output = response.text.strip().strip(\"```json\").strip(\"```\")\n",
    "            parsed = json.loads(raw_output)\n",
    "            results.append(parsed)\n",
    "            break  # success\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\" Failed at row {idx}: {e}\")\n",
    "                results.append({})\n",
    "                fail_log.append({\n",
    "                    \"row_index\": idx,\n",
    "                    \"eligibility\": eligibility,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "# Merge & Save\n",
    "features_df = pd.DataFrame(results)\n",
    "final_df = pd.concat([chunk.reset_index(drop=True), features_df], axis=1)\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\" Saved {output_file}\")\n",
    "\n",
    "# Save failed logs\n",
    "if fail_log:\n",
    "    pd.DataFrame(fail_log).to_csv(fail_file, index=False)\n",
    "    print(f\" Saved failed prompts to {fail_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "R1r1jQIYEgru",
    "outputId": "2d844813-b2e9-4fcf-cb36-47bf13a3f3af"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing batch 3/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [50:38<00:00,  6.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved extracted_features_batch3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# ---- Process only 1st batch ---- #\n",
    "i = 2  # batch index\n",
    "chunk = chunks[i]\n",
    "print(f\"\\n Processing batch {i+1}/{len(chunks)}...\")\n",
    "\n",
    "# Check for previous progress\n",
    "output_file = f\"extracted_features_batch{i+1}.csv\"\n",
    "fail_file = f\"failed_prompts_batch{i+1}.csv\"\n",
    "\n",
    "already_done = []\n",
    "if os.path.exists(output_file):\n",
    "    existing = pd.read_csv(output_file)\n",
    "    already_done = existing.index.tolist()\n",
    "    print(f\" Resuming: {len(already_done)} rows already processed.\")\n",
    "\n",
    "results = []\n",
    "fail_log = []\n",
    "\n",
    "for idx, row in tqdm(chunk.iterrows(), total=len(chunk)):\n",
    "    if idx in already_done:\n",
    "        continue  # Skip already processed rows\n",
    "\n",
    "    eligibility = str(row[\"Eligibility\"])\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            prompt = template.format(eligibility=eligibility)\n",
    "            response = model.generate_content(prompt)\n",
    "            raw_output = response.text.strip().strip(\"```json\").strip(\"```\")\n",
    "            parsed = json.loads(raw_output)\n",
    "            results.append(parsed)\n",
    "            break  # success\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\" Failed at row {idx}: {e}\")\n",
    "                results.append({})\n",
    "                fail_log.append({\n",
    "                    \"row_index\": idx,\n",
    "                    \"eligibility\": eligibility,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "# Merge & Save\n",
    "features_df = pd.DataFrame(results)\n",
    "final_df = pd.concat([chunk.reset_index(drop=True), features_df], axis=1)\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\" Saved {output_file}\")\n",
    "\n",
    "# Save failed logs\n",
    "if fail_log:\n",
    "    pd.DataFrame(fail_log).to_csv(fail_file, index=False)\n",
    "    print(f\" Saved failed prompts to {fail_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MaO5wO4wvS9_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X8pXRXiyDtSx"
   },
   "source": [
    "# **Batches**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tA5cCbWkD21M",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Batch 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "id": "R5PKorRvDqs3",
    "outputId": "bfcce221-205e-4864-bc62-4513cdc45fb0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing batch 1/8...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 500/500 [52:47<00:00,  6.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved extracted_features_batch1_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "i = 0  # batch index\n",
    "chunk = chunks[i]\n",
    "print(f\"\\n Processing batch {i+1}/{len(chunks)}...\")\n",
    "\n",
    "# Check for previous progress\n",
    "output_file = f\"extracted_features_batch{i+1}_new.csv\"\n",
    "fail_file = f\"failed_prompts_batch{i+1}_new.csv\"\n",
    "\n",
    "already_done = []\n",
    "if os.path.exists(output_file):\n",
    "    existing = pd.read_csv(output_file)\n",
    "    already_done = existing.index.tolist()\n",
    "    print(f\" Resuming: {len(already_done)} rows already processed.\")\n",
    "\n",
    "results = []\n",
    "fail_log = []\n",
    "\n",
    "for idx, row in tqdm(chunk.iterrows(), total=len(chunk)):\n",
    "    if idx in already_done:\n",
    "        continue  # Skip already processed rows\n",
    "\n",
    "    eligibility = str(row[\"Eligibility\"])\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            prompt = template.format(eligibility=eligibility)\n",
    "            response = model.generate_content(prompt)\n",
    "            raw_output = response.text.strip().strip(\"```json\").strip(\"```\")\n",
    "            parsed = json.loads(raw_output)\n",
    "            results.append(parsed)\n",
    "            break  # success\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\" Failed at row {idx}: {e}\")\n",
    "                results.append({})\n",
    "                fail_log.append({\n",
    "                    \"row_index\": idx,\n",
    "                    \"eligibility\": eligibility,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "# Merge & Save\n",
    "features_df = pd.DataFrame(results)\n",
    "final_df = pd.concat([chunk.reset_index(drop=True), features_df], axis=1)\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\" Saved {output_file}\")\n",
    "\n",
    "# Save failed logs\n",
    "if fail_log:\n",
    "    pd.DataFrame(fail_log).to_csv(fail_file, index=False)\n",
    "    print(f\" Saved failed prompts to {fail_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AK4_NcwdMDSn",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Batch 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "82UPb5B0EEBu"
   },
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "chunks = [df[i:i+batch_size] for i in range(0, len(df), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qwpCI-nPMI76",
    "outputId": "f38d51ae-5639-4b66-8b6d-ed38990e9db4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing batch 2/4...\n",
      " Resuming: 500 rows already processed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1000/1000 [1:38:37<00:00,  5.92s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved extracted_features_batch2.csv\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "chunk = chunks[i]\n",
    "print(f\"\\n Processing batch {i+1}/{len(chunks)}...\")\n",
    "\n",
    "# Check for previous progress\n",
    "output_file = f\"extracted_features_batch{i+1}.csv\"\n",
    "fail_file = f\"failed_prompts_batch{i+1}.csv\"\n",
    "\n",
    "already_done = []\n",
    "if os.path.exists(output_file):\n",
    "    existing = pd.read_csv(output_file)\n",
    "    already_done = existing.index.tolist()\n",
    "    print(f\" Resuming: {len(already_done)} rows already processed.\")\n",
    "\n",
    "results = []\n",
    "fail_log = []\n",
    "\n",
    "for idx, row in tqdm(chunk.iterrows(), total=len(chunk)):\n",
    "    if idx in already_done:\n",
    "        continue  # Skip already processed rows\n",
    "\n",
    "    eligibility = str(row[\"Eligibility\"])\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            prompt = template.format(eligibility=eligibility)\n",
    "            response = model.generate_content(prompt)\n",
    "            raw_output = response.text.strip().strip(\"```json\").strip(\"```\")\n",
    "            parsed = json.loads(raw_output)\n",
    "            results.append(parsed)\n",
    "            break  # success\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\" Failed at row {idx}: {e}\")\n",
    "                results.append({})\n",
    "                fail_log.append({\n",
    "                    \"row_index\": idx,\n",
    "                    \"eligibility\": eligibility,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "# Merge & Save\n",
    "features_df = pd.DataFrame(results)\n",
    "final_df = pd.concat([chunk.reset_index(drop=True), features_df], axis=1)\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\" Saved {output_file}\")\n",
    "\n",
    "# Save failed logs\n",
    "if fail_log:\n",
    "    pd.DataFrame(fail_log).to_csv(fail_file, index=False)\n",
    "    print(f\" Saved failed prompts to {fail_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nA3hFAt8OGww",
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Batch 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1100\n",
    "chunks = [df[i:i+batch_size] for i in range(0, len(df), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing batch 3/4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1100/1100 [1:46:13<00:00,  5.79s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved extracted_features_batch3_new.csv\n"
     ]
    }
   ],
   "source": [
    "i = 2\n",
    "chunk = chunks[i]\n",
    "print(f\"\\n Processing batch {i+1}/{len(chunks)}...\")\n",
    "\n",
    "# Check for previous progress\n",
    "output_file = f\"extracted_features_batch{i+1}_new.csv\"\n",
    "fail_file = f\"failed_prompts_batch{i+1}_new.csv\"\n",
    "\n",
    "already_done = []\n",
    "if os.path.exists(output_file):\n",
    "    existing = pd.read_csv(output_file)\n",
    "    already_done = existing.index.tolist()\n",
    "    print(f\" Resuming: {len(already_done)} rows already processed.\")\n",
    "\n",
    "results = []\n",
    "fail_log = []\n",
    "\n",
    "for idx, row in tqdm(chunk.iterrows(), total=len(chunk)):\n",
    "    if idx in already_done:\n",
    "        continue  # Skip already processed rows\n",
    "\n",
    "    eligibility = str(row[\"Eligibility\"])\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            prompt = template.format(eligibility=eligibility)\n",
    "            response = model.generate_content(prompt)\n",
    "            raw_output = response.text.strip().strip(\"```json\").strip(\"```\")\n",
    "            parsed = json.loads(raw_output)\n",
    "            results.append(parsed)\n",
    "            break  # success\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\" Failed at row {idx}: {e}\")\n",
    "                results.append({})\n",
    "                fail_log.append({\n",
    "                    \"row_index\": idx,\n",
    "                    \"eligibility\": eligibility,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "# Merge & Save\n",
    "features_df = pd.DataFrame(results)\n",
    "final_df = pd.concat([chunk.reset_index(drop=True), features_df], axis=1)\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\" Saved {output_file}\")\n",
    "\n",
    "# Save failed logs\n",
    "if fail_log:\n",
    "    pd.DataFrame(fail_log).to_csv(fail_file, index=False)\n",
    "    print(f\" Saved failed prompts to {fail_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Batch 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1100\n",
    "chunks = [df[i:i+batch_size] for i in range(0, len(df), batch_size)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing batch 4/4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 401/401 [38:14<00:00,  5.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved extracted_features_batch4_new.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "i = 3\n",
    "chunk = chunks[i]\n",
    "print(f\"\\n Processing batch {i+1}/{len(chunks)}...\")\n",
    "\n",
    "# Check for previous progress\n",
    "output_file = f\"extracted_features_batch{i+1}_new.csv\"\n",
    "fail_file = f\"failed_prompts_batch{i+1}_new.csv\"\n",
    "\n",
    "already_done = []\n",
    "if os.path.exists(output_file):\n",
    "    existing = pd.read_csv(output_file)\n",
    "    already_done = existing.index.tolist()\n",
    "    print(f\" Resuming: {len(already_done)} rows already processed.\")\n",
    "\n",
    "results = []\n",
    "fail_log = []\n",
    "\n",
    "for idx, row in tqdm(chunk.iterrows(), total=len(chunk)):\n",
    "    if idx in already_done:\n",
    "        continue  # Skip already processed rows\n",
    "\n",
    "    eligibility = str(row[\"Eligibility\"])\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            prompt = template.format(eligibility=eligibility)\n",
    "            response = model.generate_content(prompt)\n",
    "            raw_output = response.text.strip().strip(\"```json\").strip(\"```\")\n",
    "            parsed = json.loads(raw_output)\n",
    "            results.append(parsed)\n",
    "            break  # success\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\" Failed at row {idx}: {e}\")\n",
    "                results.append({})\n",
    "                fail_log.append({\n",
    "                    \"row_index\": idx,\n",
    "                    \"eligibility\": eligibility,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "            else:\n",
    "                time.sleep(1)\n",
    "\n",
    "        time.sleep(1)\n",
    "\n",
    "# Merge & Save\n",
    "features_df = pd.DataFrame(results)\n",
    "final_df = pd.concat([chunk.reset_index(drop=True), features_df], axis=1)\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\" Saved {output_file}\")\n",
    "\n",
    "# Save failed logs\n",
    "if fail_log:\n",
    "    pd.DataFrame(fail_log).to_csv(fail_file, index=False)\n",
    "    print(f\" Saved failed prompts to {fail_file}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Tracking and redoing missed schemes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of processed batch files\n",
    "batch_files = [\n",
    "    \"extracted_features_batch1_new.csv\",\n",
    "    \"extracted_features_batch2_new.csv\",\n",
    "    \"extracted_features_batch3_new.csv\",\n",
    "    \"extracted_features_batch4_new.csv\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and combine\n",
    "combined_df = pd.concat([pd.read_csv(file) for file in batch_files], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Combined file saved as combined_extracted_features.csv\n"
     ]
    }
   ],
   "source": [
    "# Save to a new combined CSV\n",
    "combined_df.to_csv(\"combined_extracted_features.csv\", index=False)\n",
    "print(\" Combined file saved as combined_extracted_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load full dataset\n",
    "full_df = pd.read_csv(\"Cleaned_schemes.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load processed dataset\n",
    "processed_df = pd.read_csv(\"combined_extracted_features.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total rows in full dataset: 3701\n"
     ]
    }
   ],
   "source": [
    "print(f\" Total rows in full dataset: {len(full_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remaining unprocessed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Remaining unprocessed rows: 700\n"
     ]
    }
   ],
   "source": [
    "# Identify which rows are left \n",
    "remaining_df = pd.merge(\n",
    "    full_df, processed_df[[\"URL\"]],  \n",
    "    on=\"URL\", how=\"outer\", indicator=True\n",
    ").query(\"_merge == 'left_only'\").drop(columns=[\"_merge\"])\n",
    "\n",
    "print(f\" Remaining unprocessed rows: {len(remaining_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Batch 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Processing final batch (Batch 5)...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 700/700 [1:19:56<00:00,  6.85s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Saved extracted features to: extracted_features_batch5_new.csv\n"
     ]
    }
   ],
   "source": [
    "chunk = remaining_df \n",
    "print(f\"\\n Processing final batch (Batch 5)...\")\n",
    "# Output files\n",
    "output_file = \"extracted_features_batch5_new.csv\"\n",
    "fail_file = \"failed_prompts_batch5_new.csv\"\n",
    "\n",
    "# Resume logic\n",
    "already_done = []\n",
    "if os.path.exists(output_file):\n",
    "    existing = pd.read_csv(output_file)\n",
    "    already_done = existing.index.tolist()\n",
    "    print(f\" Resuming: {len(already_done)} rows already processed.\")\n",
    "\n",
    "# Extraction\n",
    "results = []\n",
    "fail_log = []\n",
    "\n",
    "for idx, row in tqdm(chunk.iterrows(), total=len(chunk)):\n",
    "    if idx in already_done:\n",
    "        continue  # Skip already processed rows\n",
    "\n",
    "    eligibility = str(row[\"Eligibility\"])\n",
    "    retries = 3\n",
    "    for attempt in range(retries):\n",
    "        try:\n",
    "            prompt = template.format(eligibility=eligibility)\n",
    "            response = model.generate_content(prompt)\n",
    "            raw_output = response.text.strip().strip(\"```json\").strip(\"```\")\n",
    "            parsed = json.loads(raw_output)\n",
    "            results.append(parsed)\n",
    "            break  # success\n",
    "        except Exception as e:\n",
    "            if attempt == retries - 1:\n",
    "                print(f\" Failed at row {idx}: {e}\")\n",
    "                results.append({})\n",
    "                fail_log.append({\n",
    "                    \"row_index\": idx,\n",
    "                    \"eligibility\": eligibility,\n",
    "                    \"error\": str(e)\n",
    "                })\n",
    "            else:\n",
    "                time.sleep(1)  # wait before retry\n",
    "    time.sleep(1)  # throttle\n",
    "\n",
    "# Save results\n",
    "features_df = pd.DataFrame(results)\n",
    "final_df = pd.concat([chunk.reset_index(drop=True), features_df], axis=1)\n",
    "final_df.to_csv(output_file, index=False)\n",
    "print(f\" Saved extracted features to: {output_file}\")\n",
    "\n",
    "if fail_log:\n",
    "    pd.DataFrame(fail_log).to_csv(fail_file, index=False)\n",
    "    print(f\"  Saved failed prompts to: {fail_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross verify the final output csv with original csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both CSV files\n",
    "df1 = pd.read_csv(\"combined_extracted_features.csv\")\n",
    "df2 = pd.read_csv(\"extracted_features_batch5_new.csv\")\n",
    "\n",
    "# Concatenate them vertically (row-wise)\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save to a new CSV (optional)\n",
    "combined_df.to_csv(\"final_gemini_extract.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[\"URL\"].duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Total matched schemes: 3701\n",
      " Sample comparison:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>URL</th>\n",
       "      <th>Eligibility_cleaned</th>\n",
       "      <th>Eligibility_extracted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.myscheme.gov.in/schemes/fadsp1012e</td>\n",
       "      <td>The applicant should be a resident of Kerala S...</td>\n",
       "      <td>The applicant should be a resident of Kerala S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.myscheme.gov.in/schemes/icmr-pdf</td>\n",
       "      <td>ICMR-PDF is open to Indian nationals only.Fres...</td>\n",
       "      <td>ICMR-PDF is open to Indian nationals only.Fres...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.myscheme.gov.in/schemes/tkgthe</td>\n",
       "      <td>1.\\tThe applicant should be a permanent reside...</td>\n",
       "      <td>1.\\tThe applicant should be a permanent reside...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.myscheme.gov.in/schemes/skerala</td>\n",
       "      <td>The applicant should be a resident of Kerala S...</td>\n",
       "      <td>The applicant should be a resident of Kerala S...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.myscheme.gov.in/schemes/sgassobcan...</td>\n",
       "      <td>The scholarship shall be open to students belo...</td>\n",
       "      <td>The scholarship shall be open to students belo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 URL  \\\n",
       "0     https://www.myscheme.gov.in/schemes/fadsp1012e   \n",
       "1       https://www.myscheme.gov.in/schemes/icmr-pdf   \n",
       "2         https://www.myscheme.gov.in/schemes/tkgthe   \n",
       "3        https://www.myscheme.gov.in/schemes/skerala   \n",
       "4  https://www.myscheme.gov.in/schemes/sgassobcan...   \n",
       "\n",
       "                                 Eligibility_cleaned  \\\n",
       "0  The applicant should be a resident of Kerala S...   \n",
       "1  ICMR-PDF is open to Indian nationals only.Fres...   \n",
       "2  1.\\tThe applicant should be a permanent reside...   \n",
       "3  The applicant should be a resident of Kerala S...   \n",
       "4  The scholarship shall be open to students belo...   \n",
       "\n",
       "                               Eligibility_extracted  \n",
       "0  The applicant should be a resident of Kerala S...  \n",
       "1  ICMR-PDF is open to Indian nationals only.Fres...  \n",
       "2  1.\\tThe applicant should be a permanent reside...  \n",
       "3  The applicant should be a resident of Kerala S...  \n",
       "4  The scholarship shall be open to students belo...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the files\n",
    "cleaned_df = pd.read_csv(\"Cleaned_schemes.csv\")\n",
    "extracted_df = pd.read_csv(\"final_gemini_extract.csv\")\n",
    "# Check common key\n",
    "key = \"URL\"  \n",
    "\n",
    "# Merge and compare\n",
    "merged = pd.merge(cleaned_df, extracted_df, on=key, how=\"inner\", suffixes=('_cleaned', '_extracted'))\n",
    "\n",
    "print(f\" Total matched schemes: {len(merged)}\")\n",
    "print(\" Sample comparison:\")\n",
    "display(merged[[key, 'Eligibility_cleaned', 'Eligibility_extracted']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_df[\"URL\"].is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extracted_df[\"URL\"].is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merged all 5 extracted batches (batch1 to batch5)\n",
    "\n",
    "- Compared the merged data with cleaned_schemes.csv (via the URL column)\n",
    "\n",
    "- Verified that all 3701 schemes matched perfectly\n",
    "\n",
    "- extraction is 100% complete "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "M6nO3aGkvsyg",
    "xiStCQifvxWl"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python (geminienv)",
   "language": "python",
   "name": "geminienv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
